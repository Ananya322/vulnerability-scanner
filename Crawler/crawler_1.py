import os
#each website you crawl is a separate project with a separate folder
def create_dir(directory):
    if not os.path.exists(directory):
        print("Creating Project" + directory)
        os.makedirs(directory)
#create_dir('Vulnerability Scanner')


#creating new file
def create_new_file(path, data):
    a = open(path,'w')
    a.write(data) #write mode to add data to it
    a.close() # frees memeory resources to prevent data leak


#create queue and crawled files
def create_data_files(projectname, baseurl):
    queue = projectname + '/queue.txt' 
    #queue.txt is the file present in Vunerability Scanner which will contain the list of urls_set to be scanned
    crawled = projectname +'/crawled.txt'
    #crawled.txt will contain files that are already crawled
    if not os.path.isfile(queue):
        create_new_file(queue, baseurl) 
    if not os.path.isfile(crawled):
        create_new_file(crawled, '')

# adding data(link) into existing file
def append(path, data):
    with open(path, 'a') as file:
        file.write(data + '\n')

#delete data from the file
def delete_content(path):
    with open(path, 'w'):                   
        pass                                

# with sets there will be no cross overs
def file_to_set(filename):
    results = set()
    with open(filename, 'rt') as a:     
        for line in a:
            results.add(line.replace('\n','')) 
    return results

# convert set to a file
def set_to_file(urls_set, file):
    delete_content(file)
    
    for urls_set in sorted(urls_set):
        append(file, urls_set) 
