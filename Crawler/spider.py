from urllib.request import urlopen
from findlinks import LinkFinder
from crawler_1 import *


class Spider:

    #class variables are shared among all instances
    projectname = '' 
    base_url = ''
    domainname = ''
    queuefile = ''
    crawledfile = ''
    queue = set()
    crawled = set()


    def __init__(self, projectname, base_url, domainname):

        Spider.projectname = projectname
        Spider.base_url = base_url
        Spider.domainname = domainname
        Spider.queuefile = Spider.projectname + '/queue.txt'
        Spider.crawledfile = Spider.projectname + '/crawled.txt'

        self.bootfile()
        self.crawlpage('First Spider', Spider.base_url)

    @staticmethod
    def bootfile():

        create_dir(Spider.projectname)
        create_data_files(Spider.projectname, Spider.base_url)
        Spider.queue = file_to_set(Spider.queuefile)
        Spider.crawled = file_to_set(Spider.crawledfile)

    @staticmethod
    def crawlpage(spider_name, pageurl):
        #displaying what page are you crawling currently
        if pageurl not in Spider.crawled:
            print(spider_name + 'is now crawling' + pageurl)
            print('Queued files' + str(len(Spider.queue)) + '\n Crawled Files ' + str(len(Spider.crawled)))

            #collectlinks is to connect to webpage and return a set of all the links present on the webpage
            Spider.add_to_queue(Spider.collectlinks(pageurl))

            Spider.queue.remove(pageurl)
            Spider.crawled.add(pageurl)
            Spider.updatefiles()

    @staticmethod
    def collectlinks(pageurl):
        html_string = ''                #to store the converted html string

        try:
            #to make the connection and store it's response
            connected = urlopen(pageurl)

            if connected.getheader('Content-Type') == 'text/html':
                html_binary = connected.read()
                html_string = html_binary.decode('utf-8')

            finder = LinkFinder(Spider.base_url, pageurl)
            finder.feed(html_string)
        except:
            print('Error: Cannot find page')
            return set()
        return finder.page_links()

    @staticmethod
    def add_to_queue(urls):
        for url in urls:
            #check if it is already in queue or is crawled
            if url in Spider.queue:
                continue
            if url in Spider.crawled:
                continue
            if Spider.domainname not in url:                 
                continue
            #to crawl pages of that particular site only

            Spider.queue.add(url)

    @staticmethod
    def updatefiles():
        set_to_file(Spider.queue, Spider.queuefile)
        set_to_file(Spider.crawled, Spider.crawledfile)
